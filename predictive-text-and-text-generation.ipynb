{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mp285/LT9/blob/master/predictive-text-and-text-generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ziSmAXB4HWQy"
      },
      "source": [
        "# Text generation with Markovify\n",
        "\n",
        "By [Allison Parrish](http://www.decontextualize.com/)\n",
        "\n",
        "This notebook is a tour of how to generate text with Markov chains! Markov chains are a simple example of *predictive text generation*, a term I use to refer to methods of text generation that make use of statistical model that, given a certain stretch of text, *predicts* which bit of text should come next, based on probabilities learned from an existing corpus of text.\n",
        "\n",
        "The code is written in Python, but you don't really need to know Python in order to use the notebook. Everything's pre-written for you, so you can just execute the cells, making small changes to the code as needed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6cVEvLIHWQ3"
      },
      "source": [
        "## Working with text files\n",
        "\n",
        "Before we get started, we'll first need some text! Grab two [plain text files from Project Gutenberg](http://www.gutenberg.org/) (or from another source of your choice) and save them to the same directory as this notebook. (I suggest working with two files because we'll be running some code explicitly to \"compare\" two texts. Also, I think seeing two different outputs from the text generation methods discussed in this notebook will help you better understand how those methods work.) The code in the following cell loads into Python variables the contents of *two plain text files*, assigned to variables `text_a` and `text_b`. You'll need to replace the filenames with the names of the files that you downloaded, keeping the quotation marks (`\"`) intact."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "scrolled": true,
        "id": "iz06tz12HWQ4"
      },
      "outputs": [],
      "source": [
        "text_a=\"\"\"My computer keeps restarting. Everything my computer does work it restarts. I  troubleshoot to help my restarting computer but it lands me in a zero-sum place, the end all be all. The pit. How do you fix a restarting computer? When the logic board has already been placed and replace once. Why would you troubleshoot a restarting computer. I don’t even like apples. I thought apple has better quality control. They want to replace my computer with a third mind but keep all the malfunctioning parts the same. My computer keeps restarting and it does it at random times. It does it at crucial moments, like when im on zoom, or watching the finale of succession. I want my computer to get out of it’s second angsty teen phase. I want my computer to work, i dont want my computer to shut down when things get hard. I want it to persevere when im cramming a 10 page essay in one day. I just want my computer to work. Because when it gets to restart, i have to keep going, how can i keep going when theres a glitch between my progress when my computer troubleshoots itself. My computer keeps restarting.\n",
        "My computer my computer my computer it is MY computer i know this because it says so on the screen or at least it used to, MY COMPUTER, it is mine. My computer is my own but we also name computers, my computer is my self but my computer is other. My computer has a name, my computer is not like any other, there are many like it but this one is mine– yes, I know, you saw the movie too, but who are you, are you mine are you my computer is my computer you or are you called by some other name, a rose by any other. The name of the. My computer is my rose, my computer is my Rosetta, my computer is my own to name, to name, to name, to name. \n",
        "\n",
        "It claims to be a tablet but can’t withstand having the keyboard turned over on itself. It collects dust despite daily use. The mouse disappears at random intervals. I only have this laptop because it was cheaper than replacing my Macbook screen, which was more expensive than the Macbook itself bought brand new with a student discount. Every update is a little too much to handle, so blue errors are abundant. A gem of modern technology, honestly.\n",
        "\n",
        "\n",
        "A beautiful little box that holds everything. The great small box of everything with everything inside - to do lists, reading, writing, dreams, calendars, mail, receipts, tickets. You name it. A piece of my mind. I love it a lot and I appreciate its portability. I rely on it to get me through everyday and it helps me be connected to my loved ones too far to see even on a monthly basis. And yet, I also wish I relied less on it. I think I stare into the little window of light a little too long. My optometrist told me don’t forget to blink before offering me blue light glasses. My computer has low light and yellow light, I think. Or is it orange? My computer is not easy on my eyes but people have been finding ways to work around it. I think my computer is a trade off of different types of health but we’re all bought in. My computer talks to other computers without windows of light. My computer has friends, like this detachable keyboard and the handy dandy trackpad. My computer is a souped up notepad from Blues Clues and I guess that makes me (like) Steve. \n",
        "\n",
        "\n",
        "My computer is black and thin and hard.\n",
        "Sometimes I like my computer. Sometimes I feel like my computer is my overlord.\n",
        "I wonder how the stickers about intel core 17 got on my computer\n",
        "Sometimes I wonder what intel core i7 is.\n",
        "The power button sometimes looks like a 45. I wonder what is on the B-side?  \n",
        "Sometimes I think that Lenovo sounds like an alien life form. Sometimes I think Lenovo is my overlord\n",
        "I don’t know why I keep the font as times new roman when it could be anything.\n",
        "Sometimes I use my computer as a plate.\n",
        "Sometimes I use my computer as a coaster.\n",
        "Sometimes I fall asleep with my computer next to me and the dog and then it feels like part of the family.\n",
        "\n",
        "Computer is grimy That’s its most salient feature. I never properly clean it or tend to it or care for it. It’s a workhorse—I love it, but the love doesn’t result in gleaming well-kept surfaces. The microphone is no longer recognized by the system (Mac OS, who knows which version of the operating system). It just suddenly out of the blue was no longer recognized on the machine. That’s okay since I have a new machine. But I continue to prefer my old grimy one. I don’t like the rigid geometric shapes of my machine. I think the shape and the glittering steel surfaces suggest a hypersolid with low entropy when in reality it’s an energy-consuming pariah. I long for a computer that has more organic geometries. That seems to melt and transform the more energy it uses.\n",
        "\n",
        " §  The Macbook Air, an Apple computer created in 2013 I believe, is a grey computer about 13 inches. The Macbook Air has USB portals but the newer macbooks no longer have this feature. My computer and I have a very awkward relationship. I have had my computer for about seven years now and for the most part I have used it to help develop my writing and research skills. I enjoy using my computer to navigate through Microsoft, Zoom, and Google Chrome because of how the infrastructure looks and the ways it feels. I am able to keep tabs open, delete materials quickly and connect with my phone and iPad. I have been looking to get a new computer either a surface pro and another macbook but I have not done much research on either. \n",
        "\n",
        "\n",
        "I broke my ankle in the tenth grade while playing basketball. It wasn’t even the classic “breaking ankles” maneuver - I just jumped up for a rebound and landed on someone else's foot. An ankle never really heals. There’s always some small drop in performance after, something that, even if it takes a long time to show up, is forever there. That’s how I feel about my computer right now. About a month ago I let my water bottle leak into my backpack. I lost a lot of good notebooks that day which I now only use for scrap paper. The worst casualty was, of course, my computer. When I took it out of my bag for my engineering ethics class I expected for everything to be ok, for it to survive as it always has. It’s been very durable in the past. But this time, I guess I forgot to close my webcam, and some water was able to leak in. It was a mess, I was distraught. I got this computer from my cousin who had an extra one, I never would have bought such an expensive computer on my own. But I broke this one beyond repair. Now, the damage has mostly healed, all that’s left are some bright spots near the bottom. I thought about getting the screen replaced, but that would be pretty expensive. All that’s left is for me to deal with my broken ankle of a computer. \n",
        " \n",
        "my computer is cold metallic expensive capitalistic but also cherished beloved vital important necessary macbook air it was made in china but they celebrate its design in California with it I do everything and cannot live without it it sometimes overheats it is small and fits in my lap and is not heavy when I take it all around the world with me the screen has dust on it because I always have my windows open in my apartment and the dust blows in I should clean it more it is new I bought it last summer by trading in my old one\n",
        " \n",
        "The primary quality that characterizes my computer is dirty. My computer is in almost constant use. Every day I eat in front of it, and although I’m not especially messy, invariably crumbs and splashes coat my computer. My computer is actually a tablet disguised as a tiny laptop. My computer’s back camera lens is smashed, but  don’t mind because that camera never worked to begin with. I would have thought the malfunctioning of the camera were a bigger problem in the first place except that I find the concept of a rear facing camera to be perfectly idiotic to begin with. I’m not going to take photographs with a tablet.\n",
        "I’m not going to describe my current computer (aluminum, dingy plastic, everything’s worn away where I’m typing, the battery’s running out, it’s trusty but almost embarrassing at this point, it makes me think of climate change and gives me anxiety!) but the first computer I ever had, which was a Tandy TRS-80 Color Computer 2. My parents bought it for Christmas one year, I think primarily for my older brother, but I was the one who ended up using it the most. Beige plastic. Cartridges. Beep boop noises. BASIC programming language. A word processor where spaces were indicated with dots, so the process of typing in text felt a little bit like Pac-Man moving across the screen, consuming pellets. I don’t really remember what the keyboard felt like? I wish I remembered, keyboards feel really important at the moment, since I’m typing so much. I typed in example programs from the manual, and the manual got so dog-eared that the cover fell off, and then I ended up redrawing the cover art on the title page in ball-point pen.\n",
        "My computer is small and pink and weighs three pounds, like a baby. It breathes through its whirring fans but when this computer was advertised it said you almost wouldn’t be able to hear it. I can hear it though, like a beating heart. It starts burning the skin of my legs when I play minecraft for too long so I’ve decided not to do that anymore. Sometimes I’m scared of it, though, like when people tell me to put a sticker over my camera so people can’t look through at me, when I’m just trying to look at my computer.\n",
        "\n",
        "My computer is the most useful thing I own. It connects me to my friends and to my work. It is my most attended to companion. I stare at it sometimes the entire day using its many applications and accesses to complete the work required of me daily. I am always with it but love the moments when I am not. My computer has become a reminder to me that the world doesn’t stop and that tasks renew themselves before they become completed. It reminds me of all of the things I have to do with the promise of all of the things I have already done. I can’t continue to be glued to the screen but as much as I’d like never to see it again I open it day in and day out lured by the need to perform in this world and lured by the ability for my computer to get it done.\n",
        "\n",
        "Sweetheart don’t leave me, but please leave me alone. Look at you, bronzed, as though you spent all day working in the sun, working hard and getting warm. Or else, is it rose gold? The lux version of millennial pink, an inverse and a companion to that luscious color we haven’t quite allowed ourselves to believe defines a whole generation. But what else could if not such a frivolous color? Not our jobs, since so many of us don’t believe we can get those even if we tried, and not our hobbies either, since so many of us are working hard to turn our hobbies into our jobs or otherwise phasing them out as redundancies in the ever expanding world that you, tanned, bronzed, rose golden slab have opened up to us now more than ever in the past. We are one with you now, a cyborg reality. Our skin your metal. Our eyes your camera. Our hair your screen. Wedded and wrapped up in plastic, packaged together the two of us, until death or virus tears us apart, and then we’re free to buy and buy again. \n",
        "\n",
        "\n",
        "\n",
        "Friend or foe? I work in technology I work with technology but I fid it so offensive the more I learn about it, the more I learn from it. Thus my computer is a handy extension of this feeling. dkMy computer holds my life  it holds the very things I keep track of, textures and memories I want to preserve but will most likely never return to it is a safe, a repo or a scrapbook and memory lane. Odd that it has this feeling since I feel such ill will towards it. This totem of humanity gone astray but it is a totem of my humanity as well? Such a paradox. I hate you I love you right. We have these emphatic emotions towards so many things. IT is cool though. Steely and postured. It has only one way to unfold and it does so quite gracefully. Its brilliant and simple. But I hate it again. I have three of them. And an iPad. And a smartphone, All different beasts that I want to run away from. When I think of my computer I think of an island without my computer. I think of selling smoothies on the side of a tropical road. I think of experience that isn’t catalogued or saved just drifts in and out of a mental ether I have no control over.\n",
        "\n",
        "My computer is a tool that I use to write and think and learn and search. Computers are everywhere and inhabit things. They think and manipulate thought. I think I feel positively about my computer, but I haven’t thought about it that way before. It’s mostly just useful to me for work. I guess I also use it to play games, though. I like being able to program on it, even though I’m not very good at it. It makes me feel like I know how it works and can control what it does. It flips the relationship I have with it, so that I feel like I have power over it instead of it being something that I use but don’t understand. But ultimately it is just an object that I use, whether or not I can control its functions internally.\n",
        "\n",
        "\n",
        "I feel fine about my computer. I used to be really precious about the first macbook i ever owned, always putting it back in its case, careful not to get it scratched. This one got fucked up, a scratch on the front, within a few weeks of me having it and so I gave up on trying to keep it perfect but I’m still careful. I don’t feel nearly as attached to it as I do my phone. I don’t check it or touch it nearly as incessantly or desperately. But it serves me well I guess. Keeps me company throughout all my masters work. \n",
        "\n",
        "\n",
        "My laptop is the first person whom I say good morning to first when I wake up. We sleep on different surfaces, but we do sleep and wake up at the same time. I come to greet her after breakfast and then we start our daily routine. Depending on what I need to do, she becomes my library, my typewriter, my newspaper, my stereo, my television, and recently post-Covid-19 my window to the outside world. We met in June 2020, and it has been love at the first sight since then. Together we have read many books, listened to many songs, learned different skills, watched many shows, and typed many words. I love my laptop.\n",
        "\n",
        "My computer is old and hot, but not hot like hawt, but hot like oh they told me if I work really hard I get to retire. Sometimes I feel bad for computer. He breathes so hard on such small tasks. Sometimes, lately, he is hot when he hasn't been opened at all, like hasn't been used all day but here he is, laboring. I like to imagine at this point he just has his own shit to do, and is out exploring while I'm off doing other things. I would like to know what he is up to but it's not my place to ask, because labor. I just try to make sure to never leave him sitting on a pillow or whatever. I also back up the cloud now. I try to make it look like it's just a habit and not personal, but I think he knows. Oh wait, I'm writing this now. I guess he definitely knows. Hey computer.\n",
        "\n",
        "\n",
        "\n",
        "I’ve built this thing of mechanical parts,\n",
        "Arranged its guts of circuits and wires.\n",
        "RGB lights and whirring fans,\n",
        "Emissions and breath weighing heavily on us all.\n",
        " \n",
        "The metal and plastic shakes and stutters,\n",
        "Once life, then dead, now alive again,\n",
        "but a life that has not forgotten it was dead not so long ago.\n",
        " \n",
        "This thing I’ve built, of mechanical parts\n",
        "A vehicle that never moves but takes me far\n",
        "Into the depths, and just high enough to see the heights out of reach\n",
        "Is it time yet, to rip out its lungs and give it another?\n",
        "\n",
        "\n",
        "I have a whole lot to say about my computer. This computer has a legacy, you know? Actually, this was the computer my mother bought for my sister and she bought me a better one, but then my sister didn’t like her so I switched with her. I liked this one better because it was little and I’m little. It also has 16 Gigabytes of RAM, which is a solid amount for coding. I used to be a coder. I never really liked it, but I was good at it. I fell perfectly into a stereotype (I’m Sri-Lankan, so Brown) (Though I might as well not be in this room). But there are things you do because you like them and there are things you do because they will make you money and you need money to live and everybody insists you live because life is beautiful or something. This computer is what I wrote my first novel on. It was only fifty thousand-ish words, which isn’t very long but is much longer than anything else I’ve written. It has all sorts of secrets. Not real secrets, actually. Just lots and lots of pirated music. Don’t tell the government. I covered this computer in stickers and little droplets of paint from various art projects. Most importantly, this is the computer I quit coding on. I didn’t want to do it anymore, so I dropped the major. That doesn’t seem like much, but it was monumental for me. I decided I wanted to be a writer. I don’t know yet if I am a writer, but I’m sure this computer will be here when I do finally figure it out. This thing was made to last a long time. \n",
        "\"\"\"\n",
        "\n",
        "text_b=\"\"\"Freewrite B: Describe an experience (real or imaginary) that does not involve a computer at all.\n",
        "\n",
        "\n",
        "I drove 200 miles two days ago. I was not on the computer, though my car is a computer. I drove the miles, the trees went by like I was drunk and I. Speed. Fast. Flow. Curves. Spin. At the wheel. Press the gas. Hit the break. Signal, sensor. Mirrors. Screen. Camera. Needle. Revolutions. I drove the car, I drove the miles, I drove the road, I drove the route. Routine. Roulette. Rosetta.\n",
        "\n",
        "Recently something odd happened to me when I was play tennis. I was supposed to play with a friend of friend whol was looking for a tennis partner. I went down to the court, met that guy, and we started playing. I could see that he was a good player, but I was sure that I could beat him. However, my wrist started to hurt, and I couldn’t go for my shots. I had lost my main weapon and I had no idea what to do. My opponent was consistent and rarely made an unforced error, but I was making those error in double digits. I lost the first set 6 to 4, and I was down in the second set 5-0. This match could have been finished in literally four strokes, and I was not happy about it. So I decided to play more conservatively and played a game of attrition. As soon as I stopped giving my opponent free points I came back, and eventually won the second set 7-5. It was seven games in a row, and this had never happened to me.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "It was more than just milk. It was her irresponsibility, laziness, and stubbornness. My sister has one job in the morning: get ready for school. Yet, she manages to find a way to make me wait for over 15 minutes while she hogs the bathroom at 7:30am. It was more than just milk. It was the fact that she is unable to set his own alarm. That I have to flash the bedroom lights in her eyes in order to get her to wake up. It is the mindset she has to blame everyone else for her problems and act like the victim. It was more than just milk. This morning she ran out the door with a bowl in her hand. I could hear the spoon hit the edge of the bowl as she got in. I thought nothing of it and proceeded to turn on the car. I back up and go out the driveway. It was 7:40, I had to set up a lab this morning. I was in a rush. My music was keeping me calm then all of a sudden I hear a yell. The milk. It had spilled all over the back of the car. My car. The car I worked so hard to get for myself. The beautiful red car with leather seats. It was more than just milk. It was how she could blame me for turning too fast. It was how she blamed me for her irresponsibility. It was the phone call I had to make to my mother trying to explain what happened. It was more than just milk. \n",
        "\n",
        "I don’t use any technology once a week. This makes family dinners super awkward. I was home this past weekend for some big family get-togethers and it was tough. Of course, I’m used to having to do these dinners without my phone or computer but it doesn’t make it any easier. My mom likes going on what I like to call “conversation tracks” when she has nothing left to say.. So will go on a track where she pretends like she’s an old person, wondering about how the young ones are doing. There’s a track where she’d go on and non about the stock market and the economy. It’s so much easier to ignore these when I have my phone around me. Sometimes I think she likes that better too. She’s just scared of the silence. And once that day is over and we can all retreat to our digital havens, we forget about who we are when the lights are off. \n",
        " \n",
        "At Lake Artemesia just before sunset, there’s this gazebo that reflects off the surface of the water. This new gazebo, positioned perpendicular to the original, is not quite as sturdy as the water ripples through its image. Still, swimming ducks find a home here in the sideways house. Sideways roofs don’t quite do the job, but coming in almost artificial-looking pink and orange hues, at least they’re beautiful. \n",
        "\n",
        "\n",
        "I cannot think of an experience that did not involve a computer of some sort maybe not a literal computer but everything these days works on computational methods is the brain also not a computer my car has a computer the bus I take has a computer my phone is a computer my computer is a computer by grocery store checkout machine is a computer the metro train has a computer what would we do without computers what does it mean to have an experience that does not involve a computer will we always have computers\n",
        "\n",
        "\n",
        "The experience of walking among the daffodils and tulips on the first warm day in March. I love the predictable sequence in which the flowers spring up: daffodils first, then tulips, then cherry trees, pear trees, azaleas, dogwoods. In the last few years, however, that predictability has waned due to climate change: they seem to bloom in a scrambled order, as though they’ve lost their organic memories. But still: there’s enough persistence in the sequencing that feels like a human or psychological anchor: as though everything is okay in the world despite everything being decidedly not okay. So the sunshine, the daffodils, are a sort of countermeasure to everything the news gives: the aggression in the Ukraine, the breakdown of our institutions, the frightening political uncertainty. The flowers are the ballast—they help counterbalance all the global upheaval.\n",
        "\n",
        "\n",
        "The annihilation of space and time! I closed the lid and let the landscape move past me. I had to ask later on what the flowering trees were—dogwood, apparently, if they weren’t just cherry trees like you’d expect. Night time scene where the streetlights whirr by overhead and the moon sticks in the same place. As you approach New York City from any direction you have to go underground, there’s no other way to enter the city except through burrowing and poking your head up into an office building. Difficult to say if this experience isn’t mediated just like any other, though. Who is to say? I wonder what it’s like to ride a blimp? I would travel back in time and pilot an airship. By the seat of my pants. With a fanciful map, ink on parchment, and a compass pointing the way toward treasure. Airship piracy, that’s the life for me.\n",
        "\n",
        "Last night at 10 pm I had a craving for ice cream. This was unusual because it was raining and 45 degrees Fahrenheit last night. But on Saturday it was quite nice out and I was in the mood for something rich and cold and creamy, so I bought a box of chocolate éclair ice cream bars. Although really I had wanted strawberry shortcake bars and the were out so I had to settle. Any way I got home and was in a hurry and emptied only the cold things from my shopping bag. When I came back to get the rest a couple hours later I realized I had left my chocolate éclairs out at room temperature. I tore open the box to see if they were salvageable but each individually sealed bag contained only a popsicle stick floating in a lake of melted ice cream. So last night at 10 pm I could think only of the chocolate éclairs I was denied and felt that I had to correct this injustice. So I went out in the dark rainy coldness and journeyed to the grocery store. The ice cream was delicious.\n",
        "We drove hours to get down to North Carolina and collect the boxes. There were books all over the man’s house, and things all over the yard for sale. They auctioned off all his valuables, but we didn’t know who he was. The house was gorgeous. It was off of a long road outside of the town, just across the border from Virginia. We couldn’t explore it, but we got to go inside and see the middle part of the house—a glass library. The rest of the rooms surrounded it but you could see into it from most of the other rooms.\n",
        "\n",
        "Last Saturday I celebrated Passover. Every year the day Passover lands on always feels like the first real day of Spring to me. On Saturday my cousins and I went out into the backyard and got as close as we could to a bunny that we saw from the kitchen window. It was eating clovers, and we sat in the grass and watched it silently. I am 20 years old but I could feel all the younger versions of myself inside of me moving then, like a beating heart.\n",
        "\n",
        "While I was walking the other day, I passed by the Cat Café in Georgetown. I walked very slowly by this Cat Café because last summer I was wanting to go inside to buy a cat. I was very excited when I randomly noticed this store while walking but I did not go inside. Through the window, I saw about three kittens playing with young children and a view adult size cat. The Cat Café was in the lower part of the building and required customers to go downstairs to enter.\n",
        "\n",
        "\n",
        "On the subway by myself heading to Brooklyn for the first time since the pandemic. Waiting for Paul to buzz me up to his apartment, the apartment where I’ve been many times before, had many meals. The lobby looks the same, as if I was just here last week and not several years ago. And there is Paul in the doorway, somehow looking a bit more frail and a bit older than 3 years ago. More than 3 years and yet just like yesterday. He shows me his terrace first, as he always does. We admire the space where the Twin Towers used to be. He talks about new buildings going up, impinging on his view, which is all of Manhattan. I think he can probably lose a little sky without much trouble. But I don’t say this. We sit on the couch and talk about his family, my family, his work, my work, easing into the harder areas with some verbal set pieces, the jokes we've used with others and that we know work. He is republishing his 100-word short stories. I listen as the light fades.   \n",
        " It’s not quite a rainbow, since as much as rainbow is meant to evoke freedom and love, it is of course tied to certain biological and physical realities of the world. So not quite a rainbow, but a plethora of colors, sprayed as plastic pieces bolted to the walls. I climb three times a week, when my fingers aren’t sore, when the snow doesn’t scare me off the roads, when I’m not climbing more than that. The gym is welcoming and alienating. I try to introduce friends, but they beg off, “I’m too weak, I don’t have the muscle.” And potentially not, but then what is the point if not to get better? I think they think there will be someone watching and judging them. And I want to tell them that no, certainly not. See, there’s rainbows everywhere! Look at all that color on the wall! Freedom, love, openness! But just like any rainbow space, there and certain emotional realities that make their fear manifest, realize it in my own mind as well as theirs. I’m a judgmental asshole sometimes. Even when I try not to be. How could they not climb what they are climbing? Look, it’s practically a ladder! And I say to myself, “I only judge people who don’t try.” But when I determine the meaning of “try,” well then, who’s trying but the ones who are good at what they’re doing? That’s the matter with things, with these rainbow spaces.\n",
        "Did I mention the daydream of me selling smoothies on the side of a tropical road? It exists, always until it happens. I do have this fear in that daydream, that when I actualize, I realize that I was in fact running the wrong way. That maybe I should have leaned in a bit more? Maybe I should have embraced it, made love to it, fuck it really. In any case, the day dream didn’t involve the computer yet it still came from the computer. See I am thinking of something that doesn’t involve you yet I came back. Why is this? Why am I finding it so difficult to tear myself from your hands?  Someone loves me more than you. Im married. It seems this free write has found itself back in the arms of the former so I will try hard to remove you from this sanctum. This morning I went shopping for auto parts, some wax to wax my car, or rather to wax off the noticeable paint my father sprayed in the dents, the marks, the exposed bare metal to make the car seem whole though I didn’t see it that way. It felt marred. It felt wrong and not my car after all.\n",
        "A few weeks ago, I went to Looneys to drink with some of the brothers of the professional fraternity I had joined. It was nice to sit with them, although it was required of me to spend this time here. I had a drink with them and contributed to conversations that I could contribute to. I felt out of place but that feeling no longer burdens me. I sat there to myself eyeing the details of the environment. Every once in a while, Colin would turn towards me with a question to garner a laugh from the rest of them and I would entertain it with a smile and a follow up question. I can’t say the hour and a half with them was all that exciting or notable and it probably only perpetuated the idea that I am quiet but it was a break in the day where I sat and thought with only minimal stress about the environment I was in. The sun was out and setting and the people were nice and it’s better than stressing over an exam or test so it was good enough for me.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "The other day I was wandering the shore of Lake Michigan on the northernmost side of the state of Indiana. There is 40 miles of shoreline there. Indiana is the state with the smallest length of lake shoreline. The average human is 0.011 miles tall. I was dwarfed, one might say, by the smallest length of Lake Michigan shoreline in the United States.\n",
        "\n",
        "There are many shipwrecks in the history of Lake Michigan, which is not really a lake but an inland sea, one of the largest in the world, or so they say. You can imagine a ship disappearing beneath its waves and never seeing the light of day again, settling at the silty bottom where nothing is known and time and space and the infinite recursions of the known universe have no currency.\n",
        "\n",
        "There are so many ships at the bottom of Lake Michigan, the sea that is known as Lake Michigan. There are so many hopes and dreams at the bottom of the sea known as Lake Michigan, many of them belonging to immigrants who came from Scandinavia, who didn’t speak a word of English and who disappeared one calm summer night from the face of the known world. Their families in Norway and Sweden never heard from them again. Imagine such a disappearing act. They carried with them everything they had, and everything they had went straight to the bottom of the lake, never to be seen again. The perfect abyss, that is, Lake Michigan, swallowed them whole.\n",
        "\n",
        "Coming to America, their hopes stashed like baubles in their pockets, might they have imagined such a fate? \n",
        "\n",
        "Who could have imagined such a thing?\n",
        "\n",
        "\n",
        "The other day I ran into my nemesis while I was trying to buy Chik-fil-A. Lots of people don’t go to chik-fil-a because they support homophobic charities, but as a gay person I feel that eating there is an act of rebellion in it’s own way. But anyway, I was there with my kid cousin, whose not my cousin but I call him that because I don’t want to say he’s my friend(because he’s kindof a dick). We’re both Sri Lankan, which matters because my nemesis is also Sri Lankan. He’s not a dick. He’s actually pretty nice. He just supports the group of people that tried to blow up by dad back when they all lived in yaalpaanam, so he sucks and I hate him. \n",
        "\n",
        "\n",
        "I recently tried to go shopping with my girlfriend. Clothes shopping. At first it sounded like it might be really fun. A beautiful day. We’d go together. Both a new summer wardrobe. Shopping has always been charged for me and even more so since coming out and ever more so recently which I’ve talked about with my therapist. It was hot and I was uncomfortable already physically and that was off to a poor start. Then we went to the aerie store instead of stopping to get food or use the bathroom like i had wanted to. Another poor choice. I felt large. Like i didnt belong there and all the tiny white blonde girls and their mothers were staring at me. I felt like a sweaty sticky ugly mess. I wanted to hide. I started to walk away and to make myself out of sight but then I thought, i shouldnt leave her alone either because she’s probably feeling the same way only in a different direction and perhaps its even more dangerous for her. So i tried to calm down. Maybe it was in my head but honestly I dont think so. I saw their eyes. I know what I look like. Or at least in that moment I did. I think sometimes I forget how visibly queer I am until I’m in a space like that. A women’s space. I think my gauge on how “passable” someone is has also skewed entirely. I have no idea how my friends and lovers look to other people. To me they look like themselves. And then we are being stared at. Gawked. And suddenly I’m jerked back into the realization that we stick out, especially together. I cried on the ride home. I felt impossibly uncomfortable. I wanted to melt. I wanted to shed my skin. Scrub it clean. Like in the dishwasher or the washing machine. Run it through a couple of cycles. I wanted to be small. I wanted to be out of sight. I wanted to feel clean. I didn’t though. Even after I took a too hot shower and put on my dysphoria hoodie and sweatpants. She kissed me. Over the ugly tears. She didn’t know what to say and I knew that and it was okay and I felt stupid. Is it ridiculous to think that brought us closer somehow. I know what our incompatibilities are and one of them is simultaneously a compatibility. We both know what overstimulation feels like, what it feels like to stand out, what it feels like to be visibly queer and trans. When we were both in that at the same time we panicked. Too close and also too far. \n",
        "\n",
        "Since working on a computer takes most of my time, my non computer time feels especially personal. Much of my time is not computer time, but its time taken, which is different from time spent. Or maybe it is the other way around. But my relationship to my computer is a story about time, so time away from my computer is story about something else. Or maybe it's the other way around.\n",
        "\n",
        "\n",
        "I stood in front of the sink, its round basin full to the brim\n",
        "Plates, cups, and knives drenched in a residue of appeal long lost\n",
        "And to my left a sponge, still dry, a question posed in its stance\n",
        "Are you ready?\n",
        " \n",
        "I’ve learned to hate questions like these\n",
        "When it does not matter how answer\n",
        "The path forward is the same\n",
        "\n",
        "\"\"\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbYoQpLUHWQ5"
      },
      "source": [
        "These variables are *strings*, which are essentially just long lists of the characters that occur in the text, in the order that they occur. The code in the following cell shows the first two hundred characters of text A:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jbSnxwcCHWQ5",
        "outputId": "98c4569e-1706-4cf6-b4df-6ba592d13c3d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "My computer keeps restarting. Everything my computer does work it restarts. I  troubleshoot to help my restarting computer but it lands me in a zero-sum place, the end all be all. The pit. How do you \n"
          ]
        }
      ],
      "source": [
        "print(text_a[:200])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EkCwsarXHWQ6"
      },
      "source": [
        "You can change `text_a` to `text_b` to see the output from your second text, or change `200` to a number of your choosing.\n",
        "\n",
        "The `random.sample()` function gives us a random sampling of the contents of a variable (as long as that variable is a sequence of things, like a string or a list). So, for example, to see twenty random characters from text B:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vt6N_FneHWQ6",
        "outputId": "02761aa1-1fec-47f6-de80-d9c52ab27d4c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['t',\n",
              " 't',\n",
              " 'm',\n",
              " ' ',\n",
              " ' ',\n",
              " 'm',\n",
              " 'e',\n",
              " 'r',\n",
              " 'e',\n",
              " 'a',\n",
              " 'y',\n",
              " 'w',\n",
              " ' ',\n",
              " 'e',\n",
              " 'o',\n",
              " ' ',\n",
              " ' ',\n",
              " ' ',\n",
              " 'r',\n",
              " 'e']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "import random\n",
        "random.sample(text_b, 20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTh1wQfyHWQ7"
      },
      "source": [
        "This isn't incredibly helpful on its own, but you'll notice that the characters it drew (probably) more or less follow the expected letter distribution for English (i.e., lots of `e`s and `n`s and `t`s)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmu6JRkAHWQ7"
      },
      "source": [
        "Perhaps more interesting would be to see a randomly-sampled list of *words*. To do this, we'll make separate variables for the words in the text, using a Python function called `.split()`, which takes a string and turns it into a list of words contained in that string. The following cell makes two new variables that contain the words from both texts respectively:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "2qOQmGcwHWQ8"
      },
      "outputs": [],
      "source": [
        "a_words = text_a.split()\n",
        "b_words = text_b.split()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vm_DQ3cMHWQ9"
      },
      "source": [
        "Now, ten random words from both text A and text B:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "tDOwW68FHWQ9",
        "outputId": "06be5ee9-a33a-4360-bafd-86141f945226"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['listened',\n",
              " 'a',\n",
              " 'see',\n",
              " 'absence',\n",
              " 'listening',\n",
              " 'she',\n",
              " 'as',\n",
              " 'exhibit.”',\n",
              " 'Fitzwilliam,',\n",
              " 'state']"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "random.sample(a_words, 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "vJtXVJ4kHWQ9",
        "outputId": "4fd1863e-4152-486d-efe6-917ebd6c8a27"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['saw',\n",
              " 'shepherd.',\n",
              " 'only',\n",
              " 'rustic,',\n",
              " 'being',\n",
              " 'resolved',\n",
              " 'near',\n",
              " 'would',\n",
              " 'feel',\n",
              " 'with']"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "random.sample(b_words, 10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffT9h6ePHWQ-"
      },
      "source": [
        "The code in the following cell uses Python's `Counter` object to count the *most common* letters in the first of these texts:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "I4Ls_oQaHWQ-",
        "outputId": "98b9257b-959c-4910-9447-ef79d4533be0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[(' ', 113941),\n",
              " ('e', 70344),\n",
              " ('t', 47283),\n",
              " ('a', 42156),\n",
              " ('o', 41138),\n",
              " ('n', 38430),\n",
              " ('i', 36273),\n",
              " ('h', 33883),\n",
              " ('r', 33293),\n",
              " ('s', 33292),\n",
              " ('d', 22247),\n",
              " ('l', 21282)]"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from collections import Counter\n",
        "Counter(text_a).most_common(12)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IB7ZVyZTHWQ-"
      },
      "source": [
        "Specifying the `a_words` variable gives the most frequent *words* instead:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "Q86XFNFWHWQ_",
        "outputId": "0a356318-4333-4b9c-ce4d-76e55f099489"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('the', 4205),\n",
              " ('to', 4121),\n",
              " ('of', 3662),\n",
              " ('and', 3309),\n",
              " ('a', 1945),\n",
              " ('her', 1858),\n",
              " ('in', 1813),\n",
              " ('was', 1795),\n",
              " ('I', 1740),\n",
              " ('that', 1419),\n",
              " ('not', 1356),\n",
              " ('she', 1306)]"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Counter(a_words).most_common(12)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0t1r4i5EHWQ_"
      },
      "source": [
        "Compare these to the most common words in text B:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "bJ9fhloPHWQ_",
        "outputId": "2abfc1ab-14fc-4840-fe21-86ead95179bc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('the', 4056),\n",
              " ('and', 2971),\n",
              " ('of', 2741),\n",
              " ('I', 2719),\n",
              " ('to', 2142),\n",
              " ('my', 1631),\n",
              " ('a', 1394),\n",
              " ('in', 1125),\n",
              " ('was', 993),\n",
              " ('that', 987),\n",
              " ('with', 700),\n",
              " ('had', 679)]"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Counter(b_words).most_common(12)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ky8WYNoFHWQ_"
      },
      "source": [
        "## Markov models\n",
        "\n",
        "I won't go into the precise details of how to implement a Markov chain text generator in this notebook. (I have written [a tutorial on this topic](https://github.com/aparrish/rwet/blob/master/ngrams-and-markov-chains.ipynb) elsewhere, however!) But I think it's helpful to understand the fundamentals of how Markov chain text generation works. The next question we’re going to try to answer is this: Given a stretch of text (say a string of characters, or run of words), what is most the most likely bit of text to come next?\n",
        "\n",
        "One way to answer this question is with an n-gram based Markov model. What's an n-gram? I'm glad you asked!\n",
        "\n",
        "### N-grams\n",
        "\n",
        "An n-gram is simply a sequence of units drawn from a longer sequence; in the case of text, the unit in question is usually a character or a word. For convenience, we'll call the unit of the n-gram is called its level; the length of the n-gram is called its order. For example, the following is a list of all unique character-level order-2 n-grams in the word condescendences:\n",
        "\n",
        "    co\n",
        "    on\n",
        "    nd\n",
        "    de\n",
        "    es\n",
        "    sc\n",
        "    ce\n",
        "    en\n",
        "    nc\n",
        "\n",
        "And the following is an excerpt from the list of all unique word-level order-5 n-grams in The Road Not Taken:\n",
        "\n",
        "    Two roads diverged in a\n",
        "    roads diverged in a yellow\n",
        "    diverged in a yellow wood,\n",
        "    in a yellow wood, And\n",
        "    a yellow wood, And sorry\n",
        "    yellow wood, And sorry I\n",
        "\n",
        "N-grams are used frequently in natural language processing and are a basic tool text analysis. Their applications range from programs that correct spelling to creative visualizations to compression algorithms to stylometrics to generative text.\n",
        "\n",
        "### What comes next?\n",
        "\n",
        "A Markov model for text begins with a list of n-grams. But in addition to making this list, we also keep track of what unit of text (word, character, etc.) *follows* each of those n-grams.\n",
        "\n",
        "Let’s do a quick example by hand. This is the same character-level order-2 n-gram analysis of the (very brief) text “condescendences” as above, but this time keeping track of all characters that follow each n-gram:\n",
        "\n",
        "| n-grams |\tnext? |\n",
        "| ------- | ----- |\n",
        "|co| n|\n",
        "|on| d|\n",
        "|nd| e, e|\n",
        "|de| s, n|\n",
        "|es| c, (end of text)|\n",
        "|sc| e|\n",
        "|ce| n, s|\n",
        "|en| d, c|\n",
        "|nc| e|"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZLM8qk6pHWRA"
      },
      "source": [
        "From this table, we can determine that while the n-gram `co` is followed by n 100% of the time, and while the n-gram `on` is followed by `d` 100% of the time, the n-gram `de` is followed by `s` 50% of the time, and `n` the rest of the time. Likewise, the n-gram `es` is followed by `c` 50% of the time, and followed by the end of the text the other 50% of the time.\n",
        "\n",
        "Exercise: Imagine (or even better, write out) what this table might look like if you were analyzing words instead of characters, with a source text of your choice."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7pLH5Z_HWRA"
      },
      "source": [
        "### Markov chains: Generating text from a Markov model\n",
        "\n",
        "The Markov models we created above don't just give us interesting statistical probabilities. It also allows us generate a *new* text with those probabilities by *chaining together predictions*. Here’s how we’ll do it, starting with the order 2 character-level Markov model of `condescendences`: (1) start with the initial n-gram (`co`)—those are the first two characters of our output. (2) Now, look at the last *n* characters of output, where *n* is the order of the n-grams in our table, and find those characters in the “n-grams” column. (3) Choose randomly among the possibilities in the corresponding “next” column, and append that letter to the output. (Sometimes, as with `co`, there’s only one possibility). (4) If you chose “end of text,” then the algorithm is over. Otherwise, repeat the process starting with (2). Here’s a record of the algorithm in action:\n",
        "\n",
        "    co\n",
        "    con\n",
        "    cond\n",
        "    conde\n",
        "    conden\n",
        "    condend\n",
        "    condendes\n",
        "    condendesc\n",
        "    condendesce\n",
        "    condendesces\n",
        "    \n",
        "As you can see, we’ve come up with a word that looks like the original word, and could even be passed off as a genuine English word (if you squint at it). From a statistical standpoint, the output of our algorithm is nearly indistinguishable from the input. This kind of algorithm—moving from one state to the next, according to a list of probabilities—is known as a Markov chain generator.\n",
        "\n",
        "### Generating with Markovify\n",
        "\n",
        "Fortunately, with the invention of digital computers, you don't have to perform this algorithm by hand! In fact, Markov chain text generation has been a pastime of poets and programmers going back [all the way to 1983](https://www.jstor.org/stable/24969024), so it should be no surprise that there are many implementations of the idea in Python that you can download and install. The one we're going to use is [Markovify](https://github.com/jsvine/markovify), a Markov chain text generation library originally developed for BuzzFeed, apparently. It comes with a lot of extra niceties that will make our lives easier, but underneath the hood, it implements an algorithm very similar to the one we just did by hand above.\n",
        "\n",
        "To install Markovify on your computer, run the cell below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "TY7AQbaaHWRA",
        "outputId": "979a3fc4-4d52-4451-9601-c79dfebb49aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: markovify in /Users/allison/opt/miniconda3/envs/predictive-text-and-text-generation/lib/python3.9/site-packages (0.8.3)\r\n",
            "Requirement already satisfied: unidecode in /Users/allison/opt/miniconda3/envs/predictive-text-and-text-generation/lib/python3.9/site-packages (from markovify) (1.1.1)\r\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "!{sys.executable} -m pip install markovify"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l389OkNpHWRA"
      },
      "source": [
        "And then run this cell to make the library available in your notebook:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "tib7acOAHWRB"
      },
      "outputs": [],
      "source": [
        "import markovify"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GrUtcBdbHWRB"
      },
      "source": [
        "The code in the following cell creates a new text generator, using the text in the variable specified to build the Markov model, which is then assigned to the variable `generator_a`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "r418ByszHWRB"
      },
      "outputs": [],
      "source": [
        "generator_a = markovify.Text(text_a)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qe2icPnhHWRB"
      },
      "source": [
        "You can then call the `.make_sentence()` method to generate a sentence from the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "vLuaR1ZcHWRB",
        "outputId": "d5bb40cb-83c5-4a00-90c1-a1c393b5e9e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "“I want to see his sister; and, oh! how ardently did she say?”\n"
          ]
        }
      ],
      "source": [
        "print(generator_a.make_sentence())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22eCZaBaHWRC"
      },
      "source": [
        "The `.make_short_sentence()` method allows you to specify a maximum length for the generated sentence:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "nnedqWq1HWRC",
        "outputId": "5a3e51db-2c17-4fbd-8965-d619d009b033"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "As long as possible.\n"
          ]
        }
      ],
      "source": [
        "print(generator_a.make_short_sentence(50))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xlq2TFoBHWRC"
      },
      "source": [
        "By default, Markovify tries to generate a sentence that is significantly different from any existing sentence in the input text. As a consequence, sometimes the `.make_sentence()` or `.make_short_sentence()` methods will return `None`, which means that in ten tries it wasn't able to generate such a sentence. You can work around this by increasing the number of times it tries to generate a sufficiently unique sentence using the `tries` parameter:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "Q7ezPKxhHWRC",
        "outputId": "5c115b8d-8d20-42d2-d45f-0aa5d2f2bad9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Elizabeth was forced to rejoice.\n"
          ]
        }
      ],
      "source": [
        "print(generator_a.make_short_sentence(40, tries=100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wOKxvoPfHWRC"
      },
      "source": [
        "Or by disabling the check altogether with `test_output=False` (note that this means the generator will occasionally return stretches of text that are present in the source text):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "Hzl8UX_LHWRC",
        "outputId": "d219797f-998b-4182-9c8b-7d77b566431f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mrs. Bennet, “that is rather singular.”\n"
          ]
        }
      ],
      "source": [
        "print(generator_a.make_short_sentence(40, test_output=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GeonuzI4HWRD"
      },
      "source": [
        "### Changing the order\n",
        "\n",
        "When you create the model, you can specify the order of the model using the `state_size` parameter. It defaults to 2. Let's make two model with different orders and compare:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "iaF6fbNgHWRD"
      },
      "outputs": [],
      "source": [
        "gen_a_1 = markovify.Text(text_a, state_size=1)\n",
        "gen_a_4 = markovify.Text(text_a, state_size=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "GCS5WfeEHWRD",
        "outputId": "c9ddfc79-cd37-4551-9d74-710c3770b787"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "order 1\n",
            "“_My_ overhearings were miserable.\n",
            "\n",
            "order 4\n",
            "As they walked across the hall towards the river, Elizabeth turned back to look again; her uncle and aunt had already lost three days of happiness, and immediately wrote as follows: “I would have thanked you before, my dear aunt, as I ought to have done, for your long, kind, satisfactory, detail of particulars; but to say the truth, I was too cross to write.\n"
          ]
        }
      ],
      "source": [
        "print(\"order 1\")\n",
        "print(gen_a_1.make_sentence(test_output=False))\n",
        "print()\n",
        "print(\"order 4\")\n",
        "print(gen_a_4.make_sentence(test_output=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amulJMPpHWRD"
      },
      "source": [
        "In general, the higher the order, the more the sentences will seem \"coherent\" (i.e., more closely resembling the source text). Lower order models will produce more variation. Deciding on the order is usually a matter of taste and trial-and-error."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltOJM5gaHWRD"
      },
      "source": [
        "### Changing the level\n",
        "\n",
        "Markovify, by default, works with *words* as the individual unit. It doesn't come out-of-the-box with support for character-level models. The following code defines a new kind of Markovify generator that implements character-level models. Execute it before continuing:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "bJF5ywo-HWRE"
      },
      "outputs": [],
      "source": [
        "class SentencesByChar(markovify.Text):\n",
        "    def word_split(self, sentence):\n",
        "        return list(sentence)\n",
        "    def word_join(self, words):\n",
        "        return \"\".join(words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lEtX3mbxHWRE"
      },
      "source": [
        "Any of the parameters you passed to `markovify.Text` you can also pass to `SentencesByChar`. The `state_size` parameter still controls the order of the model, but now the n-grams are characters, not words.\n",
        "\n",
        "The following cell implements a character-level Markov text generator for the word \"condescendences\":"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "nM3_TvcpHWRF"
      },
      "outputs": [],
      "source": [
        "con_model = SentencesByChar(\"condescendences\", state_size=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNF563OMHWRF"
      },
      "source": [
        "Execute the cell below to see the output—it'll be a lot like what we implemented by hand earlier!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "vOrEVVxxHWRF",
        "outputId": "2aa6cd21-05a3-4d5b-e1b6-13df1a7f387b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'condescendescences'"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "con_model.make_sentence()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDzJf061HWRF"
      },
      "source": [
        "Of course, you can use a character-level model on any text of your choice. So, for example, the following cell creates a character-level order-7 Markov chain text generator from text A:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "f9L_wR0tHWRF"
      },
      "outputs": [],
      "source": [
        "gen_a_char = SentencesByChar(text_a, state_size=7)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5laI-qJ4HWRF"
      },
      "source": [
        "And the cell below prints out a random sentence from this generator. (The `.replace()` is to get rid of any newline characters in the output.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "keftdwXEHWRF",
        "outputId": "a12cc77e-a96f-4b13-f002-b585c5090888"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "As I must acknowledged, however, they can get.\n"
          ]
        }
      ],
      "source": [
        "print(gen_a_char.make_sentence(test_output=False).replace(\"\\n\", \" \"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6FcQW_9HWRF"
      },
      "source": [
        "### Combining models\n",
        "\n",
        "Markovify has a handy feature that allows you to *combine* models, creating a new model that draws on probabilities from both of the source models. You can use this to create hybrid output that mixes the style and content of two (or more!) different source texts. To do this, you need to create the models independently, and then call `.combine()` to combine them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "-4E73a9WHWRG"
      },
      "outputs": [],
      "source": [
        "generator_a = markovify.Text(text_a)\n",
        "generator_b = markovify.Text(text_b)\n",
        "combo = markovify.combine([generator_a, generator_b], [0.5, 0.5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDAHtwa2HWRG"
      },
      "source": [
        "The bit of code `[0.5, 0.5]` controls the \"weights\" of the models, i.e., how much to emphasize the probabilities of any model. You can change this to suit your tastes. (E.g., if you want mostly text A with but a *soupçon* of text B, you would write `[0.9, 0.1]`. Try it!) \n",
        "\n",
        "Then you can create sentences using the combined model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "U-U8PjDFHWRG",
        "outputId": "c8cb7f5e-7689-451f-ecec-f3da06d5a310"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "We all know that his religion and taught to look on the subject, but that the arguments with which I feared to meet her again.\n"
          ]
        }
      ],
      "source": [
        "print(combo.make_sentence())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Si1ZuoePHWRG"
      },
      "source": [
        "### Bringing it all together\n",
        "\n",
        "I've pre-written some code below to make it easy for you to experiment and produce output from Markovify. Just make adjustments to the values assigned to the variables in the cell below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "3YXPNqOAHWRG"
      },
      "outputs": [],
      "source": [
        "# change to \"word\" for a word-level model\n",
        "level = \"char\"\n",
        "# controls the length of the n-gram\n",
        "order = 7\n",
        "# controls the number of lines to output\n",
        "output_n = 14\n",
        "# weights between the models; text A first, text B second.\n",
        "# if you want to completely exclude one model, set its corresponding value to 0\n",
        "weights = [0.5, 0.5]\n",
        "# limit sentence output to this number of characters\n",
        "length_limit = 280"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOdsysBfHWRG"
      },
      "source": [
        "(The lines beginning with `#` are \"comments\"—they don't do anything, they're just there to explain what's happening in the code.)\n",
        "\n",
        "After making your changes above, run the cell below to generate text according to your parameters. Repeat as necessary until you get something you really like!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "AEZn_5ELHWRG",
        "outputId": "7dd9f4ec-fedb-4a6f-dd15-a1154f55ea9a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "He was really instructed and repeatedly turned to Charlotte's first began.\n",
            "\n",
            "My ancestors had manifested that would yet had been brought to provoke Darcy should think it a fainted.\n",
            "\n",
            "Women fainted.\n",
            "\n",
            "The completed; and its extreme profundity, until I almost as far enough to hold this explaining.\n",
            "\n",
            "“So much mistaken in.\n",
            "\n",
            "Your tempered, miserable elopement with extremest agitation for many years after breakfast a few stars, it will be in this likely to have them before the dead at my first shock of procure.\n",
            "\n",
            "A military being perpetual fretting between the occasion requiring after _that_, I am a married, but a conversation with almost to London.\n",
            "\n",
            "“I do not give us a ball.\n",
            "\n",
            "In vain did Elizabeth ventured on me.”\n",
            "\n",
            "In a few grey hairs covered them in the lane, who married.\n",
            "\n",
            "Three years old.\n",
            "\n",
            "“What a persuade him to the probably been soon to what promises fairly; and the church.\n",
            "\n",
            "They soon cease to return Mr. Bingley's continued:  “Lizzy, let me be at home, therefore, to admire enough to endure; when a selfish pursuit?\n",
            "\n",
            "She could reflections, all out of its propositions of my truth of its usual querulous serene constructed her eyes.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "model_cls = markovify.Text if level == \"word\" else SentencesByChar\n",
        "gen_a = model_cls(text_a, state_size=order)\n",
        "gen_b = model_cls(text_b, state_size=order)\n",
        "gen_combo = markovify.combine([gen_a, gen_b], weights)\n",
        "for i in range(output_n):\n",
        "    out = gen_combo.make_short_sentence(length_limit, test_output=False)\n",
        "    out = out.replace(\"\\n\", \" \")\n",
        "    print(out)\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZ3VVt5KHWRG"
      },
      "source": [
        "## Generating with non-prose text\n",
        "\n",
        "Markovify assumes you're feeding it prose, i.e., a text file that can be parsed into sentences by separating on sentence-ending punctuation. But often you're *not* working with text like this. For example, let's generate some sonnets. First, download [this plaintext version of Shakespeare's sonnets](https://raw.githubusercontent.com/aparrish/plaintext-example-files/master/sonnets.txt) and keep it in the same directory as this notebook. We'll define the sonnet-generating task as consisting of (a) training a Markov chain on lines of poetry and then (b) generating a sequence of fourteen lines of poetry. Since the *line* is the unit now and not the *sentence*, we need to use Markovify's `NewlineText` class instead of `Text`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KjRwWM6EHWRH"
      },
      "outputs": [],
      "source": [
        "sonnets_text = open(\"sonnets.txt\").read()\n",
        "sonnets_model = markovify.NewlineText(sonnets_text, state_size=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2FGBhY7HWRH"
      },
      "source": [
        "And then we can generate:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W-Km5-i3HWRH",
        "outputId": "de12bfe8-d37a-453a-a1d4-9fdd31089b31"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"For thy light's flame to make time's waste:\""
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sonnets_model.make_sentence()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LaJSDPliHWRH"
      },
      "source": [
        "And now make a sonnet, sorta:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ogCKePWcHWRH",
        "outputId": "9582c037-9b2b-4b1c-e1fc-ee6525177ea0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Thy pity like a face?\n",
            "Or bends with all those.\n",
            "Although thou art forced to thee, when he may not so of self-doing crime.\n",
            "Could make you see aright?\n",
            "For thee, as thou wilt look,\n",
            "And therefore love, when first my verse alone kingdoms of self-doing crime.\n",
            "Save breed, to say you a league is she might the gaudy spring,\n",
            "The fairest creatures broke away,\n",
            "Oaths of yours must strive\n",
            "Therefore I fear,\n",
            "Deserves the subject that thy years told:\n",
            "Then happy me! but yet, like a seeting bath, which methinks I should example where I am to my love, that sorrow, which gives thee with winter hath taught it thy beauty herself is so long,\n",
            "I better in thee to be forgot,\n",
            "To give back again is took,\n"
          ]
        }
      ],
      "source": [
        "for i in range(14):\n",
        "    print(sonnets_model.make_sentence())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Li7pD941HWRH"
      },
      "source": [
        "Doing this with a character-level model is a bit more tricky. I've written code in the cell below that defines a new class, `LinesByCharacter` that works like `NewlineText` but operates character-by-character instead of word-by-word:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qplXZiMJHWRH"
      },
      "outputs": [],
      "source": [
        "class LinesByChar(markovify.NewlineText):\n",
        "    def word_split(self, sentence):\n",
        "        return list(sentence)\n",
        "    def word_join(self, words):\n",
        "        return \"\".join(words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAlj6-NJHWRH"
      },
      "source": [
        "Now we can create a character model with the sonnets, line by line:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NVeaZwAPHWRH"
      },
      "outputs": [],
      "source": [
        "sonnets_char_model = LinesByChar(sonnets_text, state_size=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3CAVon2FHWRI"
      },
      "source": [
        "And generate new sonnets:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VvrQLYsKHWRI",
        "outputId": "e2662c60-2c68-4b29-f7b3-6f09657dc232"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "One of hatred when in the be so blunt back nights in her proceeds:\n",
            "And from thy fair, and see:\n",
            "And faint of and the voice thy swear is best once with mask'd the deathese powerfumes invention, like a false with from her repair,\n",
            "Which hall the night,\n",
            "The leaven,\n",
            "For take a hours, by title morn\n",
            "The worth thee, form death desire is black lines thy deeds;\n",
            "And all upon their riot ease child me not confound age's mine eye away,\n",
            "My her.\n",
            "Since canopy disgrace image infect.\n",
            "Which now unfair witherein I do shall reness, where bore to temperfect thou away, year, when of a world's flatter stain,\n",
            "It is pleast,\n",
            "Lo! in their breat true my mother plead where fresh, who art compounds,\n",
            "Doubting lack of bloody tyrannot action but drown vision laughts, are of your critied be.\n"
          ]
        }
      ],
      "source": [
        "for i in range(14):\n",
        "    print(sonnets_char_model.make_sentence())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBjFfrykHWRI"
      },
      "source": [
        "### New moods\n",
        "\n",
        "Character-level Markov chains are especially suitable, in my experience, for generating shorter texts, like individual words or names. Let's generate names of new moods using this technique. First, download [this JSON file of moods](https://raw.githubusercontent.com/dariusk/corpora/9bb62927951f79bec2454f29d71b6e9b28d874b1/data/humans/moods.json) from [Corpora Project](https://github.com/dariusk/corpora/) and save to the same directory as this notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xSAnWqatHWRI"
      },
      "source": [
        "Then load the JSON file and grab just the list of words naming moods:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "gbey5z88HWRI"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "mood_data = json.loads(open(\"./moods.json\").read())\n",
        "moods = mood_data['moods']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZORbY0Q-HWRI"
      },
      "source": [
        "The easiest way to use this is to make one big string with the moods joined together with newlines:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dpXAtqigHWRJ"
      },
      "outputs": [],
      "source": [
        "moods_text = \"\\n\".join(moods)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOX9frEoHWRJ"
      },
      "source": [
        "Then use `LinesByChar` to make the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QcOSTEeSHWRJ"
      },
      "outputs": [],
      "source": [
        "moods_char_model = LinesByChar(moods_text, state_size=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iw5EwZ59HWRK"
      },
      "source": [
        "And voila, new moods:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wYZnrEPQHWRK",
        "outputId": "a6ee65ad-f0fc-470c-817b-a10695909506"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cowarlessive\n",
            "toughted\n",
            "lyricapative\n",
            "convictorted\n",
            "embarratived\n",
            "teasane\n",
            "feistative\n",
            "powerloaded\n",
            "bothetic\n",
            "rejuvenaccepted\n",
            "teassioned\n",
            "innovatired\n",
            "chieved\n",
            "alievoless\n",
            "incomperational\n",
            "greterpressived\n",
            "imped\n",
            "soreborn\n",
            "lethant\n",
            "jadequashamed\n",
            "intemplifeless\n",
            "deterrientalkatirical\n",
            "enconfidead\n",
            "strappallous\n"
          ]
        }
      ],
      "source": [
        "for i in range(24):\n",
        "    print(moods_char_model.make_sentence())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KeAbhOGwHWRK"
      },
      "source": [
        "## Further reading\n",
        "\n",
        "* Hayes, Brian. “Computer recreations.” Scientific American, vol. 249, no. 5, 1983, pp. 18–31. JSTOR, http://www.jstor.org/stable/24969024. (Original column from Scientific American that described how Markov chain text generation works—very readable! I can send a PDF, hit me up.)\n",
        "* [A Travesty Generator for Micros](https://elmcip.net/critical-writing/travesty-generator-micros) is a follow-up to Hayes' article that has some more theory and an actual Pascal listing (which is now mostly of only historical interest).\n",
        "* [This notebook](https://github.com/aparrish/rwet/blob/master/ngrams-and-markov-chains.ipynb) shows how to implement a Markov chain generator from scratch in Python, if you're interested in such things!\n",
        "* Lillian-Yvonne Bertram's [Travesty Generator](http://www.noemipress.org/catalog/poetry/travesty-generator/) is a striking example of Markov chains put to poetic use."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    },
    "colab": {
      "name": "predictive-text-and-text-generation.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}